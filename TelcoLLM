# 1. 필요한 라이브러리 설치
!pip install -U pyarrow==15.0.0
!pip install -U sentence-transformers langchain langchain_community langchain_chroma datasets openai transformers streamlit

# 2. 라이브러리 import 및 환경 설정
import os
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
import streamlit as st
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import Document

# OpenAI API 키 설정
OPENAI_API_KEY = "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"
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# 3. 데이터 로드 (전체 데이터 사용)
df_csv = load_dataset("bitext/Bitext-telco-llm-chatbot-training-dataset", split="train").to_pandas()[['instruction', 'response']].rename(columns={"response": "output"})
df_parquet = load_dataset("akshayjambhulkar/customer-support-telecom-alpaca", split="train").to_pandas()[['instruction', 'output']]
df = pd.concat([df_csv, df_parquet], ignore_index=True)

# 4. Fine-tuning 준비
tokenizer = AutoTokenizer.from_pretrained('gpt2')
model = AutoModelForCausalLM.from_pretrained('gpt2')

# Fine-tuning에 사용할 데이터셋 준비
def tokenize_function(examples):
    return tokenizer(examples['instruction'], padding="max_length", truncation=True, max_length=128)

train_data = df[['instruction', 'output']].rename(columns={"instruction": "input", "output": "label"}).to_dict('records')

# Hugging Face Dataset 변환
from datasets import Dataset
train_dataset = Dataset.from_pandas(pd.DataFrame(train_data))

# 토큰화
tokenized_datasets = train_dataset.map(tokenize_function, batched=True)

# TrainingArguments 설정
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=1,
    evaluation_strategy="epoch",
    logging_dir="./logs",
    save_total_limit=2,
)

# Trainer 설정
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    tokenizer=tokenizer
)

# Fine-tuning 수행
trainer.train()

# Fine-tuned 모델 저장
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

# 5. 임베딩 및 벡터 스토어 생성
docs = [f"질문: {row['instruction']}\n답변: {row['output']}" for _, row in df.iterrows()]
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = text_splitter.split_text("\n\n".join(docs))

# Document 객체로 변환
documents = [Document(page_content=split) for split in splits]

# HuggingFace 임베딩 모델 사용
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# Chroma Vector Store 생성
vectorstore = Chroma.from_documents(documents=documents, embedding=embedding_model)

# 검색기 생성
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 6. Streamlit 앱 구성 (챗봇 웹 실행)
st.title("고객센터 챗봇")
st.write("고객센터 관련 질문을 입력하세요:")

# 사용자 입력 받기
question = st.text_input("질문을 입력하세요:")

# Prompt 생성
prompt = PromptTemplate.from_template("""
당신은 통신사 고객 서비스 담당자입니다. 다음의 맥락을 참고하여 질문에 답변해주세요. 
모르는 내용이라면 모른다고 솔직히 말씀해주세요. 
최대 3문장으로 간결하게 답변해주시고, 고객에게 친절하고 공손한 어조로 대답해주세요.

질문: {question}
맥락: {context}
답변:
""")

# RAG 체인 생성
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": prompt},
)

# 질문에 대한 응답 처리 및 결과 출력
if question:
    result = rag_chain({"query": question})
    st.write(f"챗봇 답변: {result['result']}")
