# app.py

import os
import pandas as pd
import streamlit as st
from datasets import load_dataset
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import Document
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
import torch

# Streamlit ì„¤ì •
st.set_page_config(page_title="í…”ì½” ê³ ê°ì„¼í„° ì±—ë´‡", page_icon="ğŸ¤–")

# OpenAI API í‚¤ ì„¤ì • (Streamlit secrets ì‚¬ìš©)
os.environ["OPENAI_API_KEY"] = st.secrets["openai"]["api_key"]

@st.cache_resource
def load_data():
    df_csv = load_dataset("bitext/Bitext-telco-llm-chatbot-training-dataset", split="train").to_pandas()[['instruction', 'response']].rename(columns={"response": "output"}).sample(50, random_state=42)
    df_parquet = load_dataset("akshayjambhulkar/customer-support-telecom-alpaca", split="train").to_pandas()[['instruction', 'output']].sample(50, random_state=42)
    return pd.concat([df_csv, df_parquet], ignore_index=True)

@st.cache_resource
def fine_tune_model(df):
    tokenizer = AutoTokenizer.from_pretrained('gpt2')
    model = AutoModelForCausalLM.from_pretrained('gpt2')
    tokenizer.pad_token = tokenizer.eos_token

    def tokenize_function(examples):
        inputs = tokenizer(examples['input'], padding="max_length", truncation=True, max_length=128)
        labels = tokenizer(examples['label'], padding="max_length", truncation=True, max_length=128)
        labels["input_ids"] = [[-100 if token == tokenizer.pad_token_id else token for token in label] for label in labels["input_ids"]]
        inputs["labels"] = labels["input_ids"]
        return inputs

    train_data = df[['instruction', 'output']].rename(columns={"instruction": "input", "output": "label"}).to_dict('records')
    train_dataset = load_dataset('dict', data={'train': train_data})['train']
    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)

    training_args = TrainingArguments(
        output_dir="./results",
        per_device_train_batch_size=4,
        num_train_epochs=1,
        evaluation_strategy="no",
        logging_dir="./logs",
        save_total_limit=2,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets,
        tokenizer=tokenizer,
    )

    trainer.train()
    model.save_pretrained("./fine_tuned_model")
    tokenizer.save_pretrained("./fine_tuned_model")
    return model, tokenizer

@st.cache_resource
def create_vectorstore(df):
    docs = [f"ì§ˆë¬¸: {row['instruction']}\në‹µë³€: {row['output']}" for _, row in df.iterrows()]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_text("\n\n".join(docs))
    documents = [Document(page_content=split) for split in splits]
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    return Chroma.from_documents(documents=documents, embedding=embedding_model)

def main():
    st.title("í…”ì½” ê³ ê°ì„¼í„° ì±—ë´‡ ğŸ¤–")
    st.write("ê³ ê°ì„¼í„° ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    df = load_data()
    model, tokenizer = fine_tune_model(df)
    vectorstore = create_vectorstore(df)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

    prompt = PromptTemplate.from_template("""
    ë‹¹ì‹ ì€ í†µì‹ ì‚¬ ê³ ê° ì„œë¹„ìŠ¤ ë‹´ë‹¹ìì…ë‹ˆë‹¤. ë‹¤ìŒì˜ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. 
    ëª¨ë¥´ëŠ” ë‚´ìš©ì´ë¼ë©´ ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§ì”€í•´ì£¼ì„¸ìš”. 
    ìµœëŒ€ 3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì‹œê³ , ê³ ê°ì—ê²Œ ì¹œì ˆí•˜ê³  ê³µì†í•œ ì–´ì¡°ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: {question}
    ë§¥ë½: {context}
    ë‹µë³€:
    """)

    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.3)
    rag_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
    )

    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")
    if question:
        with st.spinner('ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...'):
            result = rag_chain({"query": question})
            st.write(f"ì±—ë´‡ ë‹µë³€: {result['result']}")

if __name__ == "__main__":
    main()
